{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'C:\\Users\\Aaron\\OneDrive\\Desktop\\nlp-getting-started\\test.csv')\n",
    "df_train = pd.read_csv(r'C:\\Users\\Aaron\\OneDrive\\Desktop\\nlp-getting-started\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing Train Data\n",
    "import re\n",
    "import string\n",
    "\n",
    "#Remove URLS\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "t\n",
      "@bbcmtd Wholesale Markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in df_train.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "df_train[\"text\"] = df_train.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "df_train[\"text\"] = df_train.text.map(remove_punct)\n",
    "\n",
    "#Test\n",
    "df_test[\"text\"] = df_test.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "df_test[\"text\"] = df_test.text.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the stop words\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_nonwords(text):\n",
    "    filtered_words = [w for w in nltk.wordpunct_tokenize(text) if w.lower() in words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text\"] = df_train.text.map(remove_stopwords)\n",
    "#df_train[\"text\"] = df_train.text.map(remove_nonwords)\n",
    "df_test[\"text\"] = df_test.text.map(remove_stopwords)\n",
    "#df_test[\"text\"] = df_test.text.map(remove_nonwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "#df_train_word = df_train.text.append(df_test.text)\n",
    "counter = counter_word(df_train.text) #Vocab from train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17971"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counters = dict((k, v) for k, v in counter.items() if v <2)\n",
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "X = df_train.text.to_numpy()\n",
    "y = df_train.target.to_numpy()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(X_train) # fit only to training WHY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14994"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to number sequence (Train data)\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "val_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Test Data\n",
    "test_seq = tokenizer.texts_to_sequences(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the same legnth for every sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 20\n",
    "\n",
    "train_padded = pad_sequences(train_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape\n",
    "\n",
    "test_padded = pad_sequences(test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_62 (Embedding)     (None, 20, 32)            575072    \n",
      "_________________________________________________________________\n",
      "simple_rnn_31 (SimpleRNN)    (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 581,345\n",
      "Trainable params: 581,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Simple RNN model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words,32, input_length=max_length))\n",
    "model.add(layers.SimpleRNN(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "179/179 - 5s - loss: 0.5724 - accuracy: 0.7085 - val_loss: 0.4925 - val_accuracy: 0.7931\n",
      "Epoch 2/20\n",
      "179/179 - 1s - loss: 0.2581 - accuracy: 0.9047 - val_loss: 0.5536 - val_accuracy: 0.7663\n",
      "Epoch 3/20\n",
      "179/179 - 2s - loss: 0.1102 - accuracy: 0.9651 - val_loss: 0.6708 - val_accuracy: 0.7458\n",
      "Epoch 4/20\n",
      "179/179 - 2s - loss: 0.0689 - accuracy: 0.9755 - val_loss: 0.7767 - val_accuracy: 0.7616\n",
      "Epoch 5/20\n",
      "179/179 - 2s - loss: 0.0493 - accuracy: 0.9807 - val_loss: 0.8562 - val_accuracy: 0.7574\n",
      "Epoch 6/20\n",
      "179/179 - 1s - loss: 0.0456 - accuracy: 0.9802 - val_loss: 0.7745 - val_accuracy: 0.7558\n",
      "Epoch 7/20\n",
      "179/179 - 2s - loss: 0.0360 - accuracy: 0.9849 - val_loss: 0.8569 - val_accuracy: 0.7463\n",
      "Epoch 8/20\n",
      "179/179 - 1s - loss: 0.0331 - accuracy: 0.9846 - val_loss: 0.8562 - val_accuracy: 0.7237\n",
      "Epoch 9/20\n",
      "179/179 - 1s - loss: 0.0312 - accuracy: 0.9858 - val_loss: 0.9262 - val_accuracy: 0.7432\n",
      "Epoch 10/20\n",
      "179/179 - 2s - loss: 0.0285 - accuracy: 0.9865 - val_loss: 0.9468 - val_accuracy: 0.7437\n",
      "Epoch 11/20\n",
      "179/179 - 2s - loss: 0.0307 - accuracy: 0.9853 - val_loss: 0.9265 - val_accuracy: 0.7442\n",
      "Epoch 12/20\n",
      "179/179 - 1s - loss: 0.0295 - accuracy: 0.9869 - val_loss: 0.9862 - val_accuracy: 0.7605\n",
      "Epoch 13/20\n",
      "179/179 - 1s - loss: 0.0298 - accuracy: 0.9869 - val_loss: 0.9328 - val_accuracy: 0.7447\n",
      "Epoch 14/20\n",
      "179/179 - 1s - loss: 0.0320 - accuracy: 0.9844 - val_loss: 0.9104 - val_accuracy: 0.7374\n",
      "Epoch 15/20\n",
      "179/179 - 1s - loss: 0.0272 - accuracy: 0.9848 - val_loss: 0.9778 - val_accuracy: 0.7468\n",
      "Epoch 16/20\n",
      "179/179 - 1s - loss: 0.0233 - accuracy: 0.9876 - val_loss: 0.9757 - val_accuracy: 0.7673\n",
      "Epoch 17/20\n",
      "179/179 - 1s - loss: 0.0234 - accuracy: 0.9881 - val_loss: 0.9787 - val_accuracy: 0.7468\n",
      "Epoch 18/20\n",
      "179/179 - 1s - loss: 0.0290 - accuracy: 0.9865 - val_loss: 0.9114 - val_accuracy: 0.7595\n",
      "Epoch 19/20\n",
      "179/179 - 1s - loss: 0.0246 - accuracy: 0.9870 - val_loss: 0.9752 - val_accuracy: 0.7489\n",
      "Epoch 20/20\n",
      "179/179 - 1s - loss: 0.0243 - accuracy: 0.9869 - val_loss: 0.9688 - val_accuracy: 0.7600\n",
      "0:00:31.971286\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "model.fit(train_padded, y_train, epochs=20, validation_data=(val_padded, y_test), verbose=2)\n",
    "\n",
    "print(datetime.datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.where(model.predict(test_padded) > 0.5, 1, 0)\n",
    "pd.DataFrame(B).to_csv('modelSimpleRNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_63 (Embedding)     (None, 20, 32)            575072    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 599,969\n",
      "Trainable params: 599,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM model\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "179/179 - 4s - loss: 0.5599 - accuracy: 0.7024 - val_loss: 0.4602 - val_accuracy: 0.8067\n",
      "Epoch 2/20\n",
      "179/179 - 2s - loss: 0.2909 - accuracy: 0.8893 - val_loss: 0.4851 - val_accuracy: 0.7852\n",
      "Epoch 3/20\n",
      "179/179 - 2s - loss: 0.1526 - accuracy: 0.9485 - val_loss: 0.6460 - val_accuracy: 0.7621\n",
      "Epoch 4/20\n",
      "179/179 - 5s - loss: 0.1010 - accuracy: 0.9683 - val_loss: 0.7350 - val_accuracy: 0.7584\n",
      "Epoch 5/20\n",
      "179/179 - 3s - loss: 0.0737 - accuracy: 0.9760 - val_loss: 0.8851 - val_accuracy: 0.7721\n",
      "Epoch 6/20\n",
      "179/179 - 2s - loss: 0.0544 - accuracy: 0.9811 - val_loss: 0.7071 - val_accuracy: 0.7752\n",
      "Epoch 7/20\n",
      "179/179 - 2s - loss: 0.0413 - accuracy: 0.9809 - val_loss: 1.1804 - val_accuracy: 0.7658\n",
      "Epoch 8/20\n",
      "179/179 - 3s - loss: 0.0323 - accuracy: 0.9841 - val_loss: 1.3946 - val_accuracy: 0.7736\n",
      "Epoch 9/20\n",
      "179/179 - 3s - loss: 0.0388 - accuracy: 0.9823 - val_loss: 1.0638 - val_accuracy: 0.7799\n",
      "Epoch 10/20\n",
      "179/179 - 2s - loss: 0.0348 - accuracy: 0.9827 - val_loss: 1.2915 - val_accuracy: 0.7684\n",
      "Epoch 11/20\n",
      "179/179 - 2s - loss: 0.0398 - accuracy: 0.9828 - val_loss: 0.8595 - val_accuracy: 0.7642\n",
      "Epoch 12/20\n",
      "179/179 - 2s - loss: 0.0342 - accuracy: 0.9837 - val_loss: 1.3884 - val_accuracy: 0.7684\n",
      "Epoch 13/20\n",
      "179/179 - 2s - loss: 0.0283 - accuracy: 0.9851 - val_loss: 1.2837 - val_accuracy: 0.7511\n",
      "Epoch 14/20\n",
      "179/179 - 2s - loss: 0.0304 - accuracy: 0.9863 - val_loss: 1.3088 - val_accuracy: 0.7726\n",
      "Epoch 15/20\n",
      "179/179 - 2s - loss: 0.0282 - accuracy: 0.9855 - val_loss: 1.2311 - val_accuracy: 0.7637\n",
      "Epoch 16/20\n",
      "179/179 - 2s - loss: 0.0261 - accuracy: 0.9867 - val_loss: 1.3211 - val_accuracy: 0.7705\n",
      "Epoch 17/20\n",
      "179/179 - 2s - loss: 0.0344 - accuracy: 0.9862 - val_loss: 1.4861 - val_accuracy: 0.7679\n",
      "Epoch 18/20\n",
      "179/179 - 2s - loss: 0.0268 - accuracy: 0.9841 - val_loss: 1.5208 - val_accuracy: 0.7689\n",
      "Epoch 19/20\n",
      "179/179 - 2s - loss: 0.0273 - accuracy: 0.9846 - val_loss: 1.3752 - val_accuracy: 0.7647\n",
      "Epoch 20/20\n",
      "179/179 - 2s - loss: 0.0273 - accuracy: 0.9869 - val_loss: 1.0906 - val_accuracy: 0.7647\n",
      "0:00:50.398558\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "model.fit(train_padded, y_train, epochs=20, validation_data=(val_padded, y_test), verbose=2)\n",
    "\n",
    "print(datetime.datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.where(model.predict(test_padded) > 0.5, 1, 0)\n",
    "pd.DataFrame(B).to_csv('modelLSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_64 (Embedding)     (None, 20, 32)            575072    \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 64)                18816     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 593,953\n",
      "Trainable params: 593,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create GRU model\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "model.add(layers.GRU(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "179/179 - 3s - loss: 0.6611 - accuracy: 0.5927 - val_loss: 0.5285 - val_accuracy: 0.7511\n",
      "Epoch 2/20\n",
      "179/179 - 2s - loss: 0.3664 - accuracy: 0.8483 - val_loss: 0.4647 - val_accuracy: 0.7910\n",
      "Epoch 3/20\n",
      "179/179 - 2s - loss: 0.1992 - accuracy: 0.9285 - val_loss: 0.5125 - val_accuracy: 0.7747\n",
      "Epoch 4/20\n",
      "179/179 - 2s - loss: 0.1127 - accuracy: 0.9618 - val_loss: 0.6527 - val_accuracy: 0.7873\n",
      "Epoch 5/20\n",
      "179/179 - 2s - loss: 0.0815 - accuracy: 0.9730 - val_loss: 0.7538 - val_accuracy: 0.7742\n",
      "Epoch 6/20\n",
      "179/179 - 2s - loss: 0.0664 - accuracy: 0.9797 - val_loss: 0.7311 - val_accuracy: 0.7820\n",
      "Epoch 7/20\n",
      "179/179 - 2s - loss: 0.0597 - accuracy: 0.9837 - val_loss: 0.7677 - val_accuracy: 0.7715\n",
      "Epoch 8/20\n",
      "179/179 - 2s - loss: 0.0541 - accuracy: 0.9809 - val_loss: 0.7293 - val_accuracy: 0.7773\n",
      "Epoch 9/20\n",
      "179/179 - 2s - loss: 0.0482 - accuracy: 0.9848 - val_loss: 0.8412 - val_accuracy: 0.7747\n",
      "Epoch 10/20\n",
      "179/179 - 2s - loss: 0.0407 - accuracy: 0.9842 - val_loss: 1.1274 - val_accuracy: 0.7637\n",
      "Epoch 11/20\n",
      "179/179 - 2s - loss: 0.0372 - accuracy: 0.9839 - val_loss: 1.1881 - val_accuracy: 0.7721\n",
      "Epoch 12/20\n",
      "179/179 - 2s - loss: 0.0310 - accuracy: 0.9860 - val_loss: 1.1667 - val_accuracy: 0.7684\n",
      "Epoch 13/20\n",
      "179/179 - 2s - loss: 0.0279 - accuracy: 0.9870 - val_loss: 1.3622 - val_accuracy: 0.7600\n",
      "Epoch 14/20\n",
      "179/179 - 2s - loss: 0.0249 - accuracy: 0.9867 - val_loss: 1.5468 - val_accuracy: 0.7679\n",
      "Epoch 15/20\n",
      "179/179 - 2s - loss: 0.0304 - accuracy: 0.9848 - val_loss: 1.2252 - val_accuracy: 0.7658\n",
      "Epoch 16/20\n",
      "179/179 - 2s - loss: 0.0282 - accuracy: 0.9862 - val_loss: 1.2150 - val_accuracy: 0.7694\n",
      "Epoch 17/20\n",
      "179/179 - 2s - loss: 0.0245 - accuracy: 0.9876 - val_loss: 1.3433 - val_accuracy: 0.7710\n",
      "Epoch 18/20\n",
      "179/179 - 5s - loss: 0.0332 - accuracy: 0.9860 - val_loss: 1.5049 - val_accuracy: 0.7621\n",
      "Epoch 19/20\n",
      "179/179 - 2s - loss: 0.0256 - accuracy: 0.9862 - val_loss: 1.4085 - val_accuracy: 0.7553\n",
      "Epoch 20/20\n",
      "179/179 - 2s - loss: 0.0227 - accuracy: 0.9876 - val_loss: 1.4196 - val_accuracy: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b2c72a8a88>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "model.fit(train_padded, y_train, epochs=20, validation_data=(val_padded, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.where(model.predict(test_padded) > 0.5, 1, 0)\n",
    "pd.DataFrame(B).to_csv('modelGRU.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
